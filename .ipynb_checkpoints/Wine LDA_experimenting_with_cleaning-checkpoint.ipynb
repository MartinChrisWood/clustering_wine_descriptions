{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an LDA model of wine descriptions,\n",
    "# how many are there \"really\"?\n",
    "\n",
    "Sure, there's 130,000 reviews of wines from around the world expounding their diverse tastes and characters, but how many \"really\" exist?  How many supergroups of wines that are essentially similar can be identified, and what's a representative example of each?  As someone who likes both red AND white wines (let's be honest; rose's just half and half) I feel uniquely qualified to answer this.  Let's perform a gid search of a large number of LDA models and find that with the best coherence & perplexity scores!  And then we'll visualise the countries of origin and the keywords that most characterise the qualities of these supergroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import re\n",
    "import random\n",
    "import pyLDAvis\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "\n",
    "from gensim.models.ldamulticore import LdaModel\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.stem import SnowballStemmer\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "# Define which stemmer to use in the pipeline later\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df = pd.read_csv(\"./data/wine-reviews/winemag-data-130k-v2.csv\")\n",
    "wine_df['desc_len'] = wine_df['description'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Data exploration (I'm not doing much!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129971"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wine_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119955"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.unique(wine_df['description']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119955"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are duplicate descriptions, so we're dropping them\n",
    "wine_df = wine_df.drop_duplicates(subset='description')\n",
    "len(wine_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'country',\n",
       " 'description',\n",
       " 'designation',\n",
       " 'points',\n",
       " 'price',\n",
       " 'province',\n",
       " 'region_1',\n",
       " 'region_2',\n",
       " 'taster_name',\n",
       " 'taster_twitter_handle',\n",
       " 'title',\n",
       " 'variety',\n",
       " 'winery',\n",
       " 'desc_len']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(wine_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>desc_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>119955.000000</td>\n",
       "      <td>119955.000000</td>\n",
       "      <td>111567.000000</td>\n",
       "      <td>119955.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>63211.429986</td>\n",
       "      <td>88.442291</td>\n",
       "      <td>35.620542</td>\n",
       "      <td>242.815939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>37506.055996</td>\n",
       "      <td>3.093029</td>\n",
       "      <td>42.107158</td>\n",
       "      <td>67.146799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>30498.500000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>197.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>62404.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>237.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>95386.500000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>283.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>129970.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>3300.000000</td>\n",
       "      <td>829.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0         points          price       desc_len\n",
       "count  119955.000000  119955.000000  111567.000000  119955.000000\n",
       "mean    63211.429986      88.442291      35.620542     242.815939\n",
       "std     37506.055996       3.093029      42.107158      67.146799\n",
       "min         0.000000      80.000000       4.000000      20.000000\n",
       "25%     30498.500000      86.000000      17.000000     197.000000\n",
       "50%     62404.000000      88.000000      25.000000     237.000000\n",
       "75%     95386.500000      91.000000      42.000000     283.000000\n",
       "max    129970.000000     100.000000    3300.000000     829.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>country</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>desc_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aromas include tropical fruit, broom, brimston...</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Kerin Oâ€™Keefe</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is ripe and fruity, a wine that is smooth...</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tart and snappy, the flavors of lime flesh and...</td>\n",
       "      <td>US</td>\n",
       "      <td>Paul Gregutt</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pineapple rind, lemon pith and orange blossom ...</td>\n",
       "      <td>US</td>\n",
       "      <td>Alexander Peartree</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Much like the regular bottling from 2012, this...</td>\n",
       "      <td>US</td>\n",
       "      <td>Paul Gregutt</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description   country  \\\n",
       "0  Aromas include tropical fruit, broom, brimston...     Italy   \n",
       "1  This is ripe and fruity, a wine that is smooth...  Portugal   \n",
       "2  Tart and snappy, the flavors of lime flesh and...        US   \n",
       "3  Pineapple rind, lemon pith and orange blossom ...        US   \n",
       "4  Much like the regular bottling from 2012, this...        US   \n",
       "\n",
       "          taster_name  desc_len  \n",
       "0       Kerin Oâ€™Keefe       172  \n",
       "1          Roger Voss       227  \n",
       "2        Paul Gregutt       186  \n",
       "3  Alexander Peartree       199  \n",
       "4        Paul Gregutt       249  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_df[[\"description\", \"country\", \"taster_name\", \"desc_len\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick utility function to pre-process the text\n",
    "def preprocess_desc(description):\n",
    "    return( [stemmer.stem(token) for token in simple_preprocess(str(description)) if token not in STOPWORDS] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB;  This step could be improved through first filtering to words that are pronouns.\n",
    "wine_df['tokens'] = wine_df['description'].apply(preprocess_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>country</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>desc_len</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aromas include tropical fruit, broom, brimston...</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Kerin Oâ€™Keefe</td>\n",
       "      <td>172</td>\n",
       "      <td>[aroma, includ, tropic, fruit, broom, brimston...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is ripe and fruity, a wine that is smooth...</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>227</td>\n",
       "      <td>[ripe, fruiti, wine, smooth, structur, firm, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tart and snappy, the flavors of lime flesh and...</td>\n",
       "      <td>US</td>\n",
       "      <td>Paul Gregutt</td>\n",
       "      <td>186</td>\n",
       "      <td>[tart, snappi, flavor, lime, flesh, rind, domi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pineapple rind, lemon pith and orange blossom ...</td>\n",
       "      <td>US</td>\n",
       "      <td>Alexander Peartree</td>\n",
       "      <td>199</td>\n",
       "      <td>[pineappl, rind, lemon, pith, orang, blossom, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Much like the regular bottling from 2012, this...</td>\n",
       "      <td>US</td>\n",
       "      <td>Paul Gregutt</td>\n",
       "      <td>249</td>\n",
       "      <td>[like, regular, bottl, come, rough, tannic, ru...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description   country  \\\n",
       "0  Aromas include tropical fruit, broom, brimston...     Italy   \n",
       "1  This is ripe and fruity, a wine that is smooth...  Portugal   \n",
       "2  Tart and snappy, the flavors of lime flesh and...        US   \n",
       "3  Pineapple rind, lemon pith and orange blossom ...        US   \n",
       "4  Much like the regular bottling from 2012, this...        US   \n",
       "\n",
       "          taster_name  desc_len  \\\n",
       "0       Kerin Oâ€™Keefe       172   \n",
       "1          Roger Voss       227   \n",
       "2        Paul Gregutt       186   \n",
       "3  Alexander Peartree       199   \n",
       "4        Paul Gregutt       249   \n",
       "\n",
       "                                              tokens  \n",
       "0  [aroma, includ, tropic, fruit, broom, brimston...  \n",
       "1  [ripe, fruiti, wine, smooth, structur, firm, t...  \n",
       "2  [tart, snappi, flavor, lime, flesh, rind, domi...  \n",
       "3  [pineappl, rind, lemon, pith, orang, blossom, ...  \n",
       "4  [like, regular, bottl, come, rough, tannic, ru...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_df[[\"description\", \"country\", \"taster_name\", \"desc_len\", \"tokens\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create TF-IDF representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary record\n",
    "dictionary = gensim.corpora.Dictionary(wine_df['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extreme values (words that are too rare, too common)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BOW model\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in wine_df['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From that create the TF-IDF model\n",
    "tfidf = gensim.models.TfidfModel(bow_corpus)\n",
    "wine_df['corpus_tfidf'] = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(0, 0.0714296427652335), (1, 0.18833774970920...\n",
       "1    [(0, 0.09114195243321198), (10, 0.063537146387...\n",
       "2    [(0, 0.07027358730085778), (33, 0.131788133771...\n",
       "3    [(3, 0.06098283869088092), (16, 0.064735606219...\n",
       "4    [(65, 0.12932848024109073), (66, 0.22354915104...\n",
       "Name: corpus_tfidf, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_df['corpus_tfidf'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Create LDA models (try a few!)\n",
    "Train-test split only, because I'm not iteratively improving anything here (it's for coursework dammit, no one cares!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(7)\n",
    "trainset, testset = train_test_split(wine_df, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tried 3 topics perplexity = -8.108477832784999 coherence = -1.9913738404379915\n",
      "tried 4 topics perplexity = -8.321668485256676 coherence = -2.073457940042304\n",
      "tried 5 topics perplexity = -8.491438428634032 coherence = -2.093669073764229\n",
      "tried 6 topics perplexity = -8.667081068196667 coherence = -2.417918194515604\n",
      "tried 7 topics perplexity = -8.799032388438205 coherence = -2.0669142281011643\n",
      "tried 8 topics perplexity = -8.931493133553642 coherence = -2.061258735919996\n",
      "tried 9 topics perplexity = -9.000474538101894 coherence = -7.858110623329277\n",
      "tried 10 topics perplexity = -9.207549730726551 coherence = -6.071534049319699\n",
      "tried 11 topics perplexity = -9.41901943626096 coherence = -6.3839109257822875\n",
      "tried 12 topics perplexity = -9.681384431653408 coherence = -9.574446175328864\n",
      "tried 13 topics perplexity = -9.956510340117955 coherence = -7.676728985428058\n",
      "tried 14 topics perplexity = -10.103046826563855 coherence = -6.146721539970917\n",
      "tried 15 topics perplexity = -10.244636554466746 coherence = -6.817448466261066\n",
      "tried 16 topics perplexity = -10.382851300091069 coherence = -6.992589177128954\n",
      "tried 17 topics perplexity = -10.53655991451321 coherence = -5.933403215841363\n",
      "tried 18 topics perplexity = -10.617581714054573 coherence = -8.237503633137669\n",
      "tried 19 topics perplexity = -10.74312322108046 coherence = -9.911515256260367\n",
      "tried 20 topics perplexity = -10.908951195447456 coherence = -7.409571714050659\n",
      "tried 21 topics perplexity = -11.035301182800099 coherence = -8.011343135524328\n",
      "tried 22 topics perplexity = -11.156071652786874 coherence = -8.678189913071082\n",
      "tried 23 topics perplexity = -11.272698100980193 coherence = -9.751229402272273\n",
      "tried 24 topics perplexity = -11.380393795594918 coherence = -9.764100089066988\n",
      "tried 25 topics perplexity = -11.532761662277688 coherence = -8.795918723270049\n",
      "tried 26 topics perplexity = -11.648002190959366 coherence = -8.13158546804073\n",
      "tried 27 topics perplexity = -11.780881253447847 coherence = -9.260100045462599\n",
      "tried 28 topics perplexity = -11.886656240736903 coherence = -8.66657826895407\n",
      "tried 29 topics perplexity = -11.987094358033206 coherence = -9.334581348167664\n",
      "tried 30 topics perplexity = -12.139825690006235 coherence = -9.538863947835242\n",
      "tried 31 topics perplexity = -12.202150005416012 coherence = -10.952574283226529\n",
      "tried 32 topics perplexity = -12.334992149792095 coherence = -9.807169341554758\n",
      "tried 33 topics perplexity = -12.469595279142117 coherence = -9.485043654662787\n",
      "tried 34 topics perplexity = -12.589629217910048 coherence = -10.131232704269479\n",
      "tried 35 topics perplexity = -12.69743347092999 coherence = -10.064694201956176\n",
      "tried 36 topics perplexity = -12.803729522442772 coherence = -10.743036748356971\n",
      "tried 37 topics perplexity = -12.960579922350096 coherence = -10.525501935203257\n",
      "tried 38 topics perplexity = -13.031929064394806 coherence = -10.248214815576787\n",
      "tried 39 topics perplexity = -13.198205797240805 coherence = -10.136256209294205\n",
      "tried 40 topics perplexity = -13.300346156596945 coherence = -10.095401527096863\n",
      "tried 41 topics perplexity = -13.407207141817246 coherence = -10.048578091688823\n",
      "tried 42 topics perplexity = -13.516103407114658 coherence = -10.46323708445843\n",
      "tried 43 topics perplexity = -13.681755595540306 coherence = -11.506375125788226\n",
      "tried 44 topics perplexity = -13.783044229319415 coherence = -11.293921614762727\n",
      "tried 45 topics perplexity = -13.890839883751607 coherence = -11.169477443248384\n",
      "tried 46 topics perplexity = -14.053382458751168 coherence = -10.973426410023658\n",
      "tried 47 topics perplexity = -14.19884879345477 coherence = -11.150918598860137\n",
      "tried 48 topics perplexity = -14.35496423948773 coherence = -11.43826004995924\n",
      "tried 49 topics perplexity = -14.458094058806747 coherence = -11.45195681842923\n",
      "tried 50 topics perplexity = -14.719247085693404 coherence = -11.2401608751016\n",
      "tried 51 topics perplexity = -14.97102520270065 coherence = -11.400915268614995\n",
      "tried 52 topics perplexity = -15.155891447051085 coherence = -11.323348978786045\n",
      "tried 53 topics perplexity = -15.376966194626677 coherence = -11.240495157367171\n",
      "tried 54 topics perplexity = -15.74306284620158 coherence = -11.552304545329056\n",
      "tried 55 topics perplexity = -16.18403323385911 coherence = -11.696183206877272\n",
      "tried 56 topics perplexity = -16.361432571632587 coherence = -11.030453964293956\n",
      "tried 57 topics perplexity = -16.80309527023606 coherence = -11.658053842196939\n",
      "tried 58 topics perplexity = -17.324045320696335 coherence = -11.365729580364905\n",
      "tried 59 topics perplexity = -17.893626196682643 coherence = -11.097156737175014\n",
      "tried 60 topics perplexity = -18.917646174318186 coherence = -10.27438016864659\n",
      "tried 61 topics perplexity = -19.204105352083577 coherence = -10.475370132819197\n",
      "tried 62 topics perplexity = -19.865320812798902 coherence = -10.103198213855073\n",
      "tried 63 topics perplexity = -21.33255808188388 coherence = -9.439451752947376\n",
      "tried 64 topics perplexity = -22.716278939394268 coherence = -8.618561546953508\n",
      "tried 65 topics perplexity = -23.771555191488947 coherence = -8.605775964856164\n",
      "tried 66 topics perplexity = -24.527126511289477 coherence = -7.866571111604111\n",
      "tried 67 topics perplexity = -27.125898808905358 coherence = -6.927024125346319\n",
      "tried 68 topics perplexity = -27.8508669149452 coherence = -6.914128230298339\n",
      "tried 69 topics perplexity = -30.332224365575826 coherence = -6.651124531963491\n",
      "tried 70 topics perplexity = -31.63553804839629 coherence = -7.3359130846805165\n",
      "tried 71 topics perplexity = -33.60762995800009 coherence = -7.11613106422777\n",
      "tried 72 topics perplexity = -37.42625532361837 coherence = -8.442071467548534\n",
      "tried 73 topics perplexity = -38.392270216188074 coherence = -8.605207371054851\n",
      "tried 74 topics perplexity = -42.697800307708256 coherence = -9.482829637585029\n",
      "tried 75 topics perplexity = -44.281637661113926 coherence = -9.759181302051386\n",
      "tried 76 topics perplexity = -44.24680869748669 coherence = -10.41705047432937\n",
      "tried 77 topics perplexity = -57.13088552661621 coherence = -12.370275492562996\n",
      "tried 78 topics perplexity = -61.27283558024508 coherence = -12.248027056083021\n",
      "tried 79 topics perplexity = -59.70183967105503 coherence = -12.312889174424178\n",
      "tried 80 topics perplexity = -64.96367185767475 coherence = -12.942911815044454\n",
      "tried 81 topics perplexity = -91.11412470807241 coherence = -14.07012124135831\n",
      "tried 82 topics perplexity = -68.75113515936513 coherence = -13.233969093048774\n",
      "tried 83 topics perplexity = -77.86713847454949 coherence = -13.686980641306864\n",
      "tried 84 topics perplexity = -86.28447052893289 coherence = -13.941009184945973\n",
      "tried 85 topics perplexity = -170.6751708984344 coherence = -14.341240217410958\n",
      "tried 86 topics perplexity = -94.76295983869592 coherence = -14.078602746302675\n",
      "tried 87 topics perplexity = -113.80014313317216 coherence = -14.21536487272963\n",
      "tried 88 topics perplexity = -176.60685729980258 coherence = -14.341240217410954\n",
      "tried 89 topics perplexity = -178.58474731445096 coherence = -14.341240217410954\n",
      "tried 90 topics perplexity = -114.52168573679526 coherence = -14.215728822913206\n",
      "tried 91 topics perplexity = -106.9746836916219 coherence = -14.212449936781585\n",
      "tried 92 topics perplexity = -184.51943969726264 coherence = -14.341240217410958\n",
      "tried 93 topics perplexity = -186.4980773925764 coherence = -14.341240217410958\n",
      "tried 94 topics perplexity = -188.4769744873021 coherence = -14.34124021741096\n",
      "tried 95 topics perplexity = -190.4561309814434 coherence = -14.34124021741096\n",
      "tried 96 topics perplexity = -192.43547058105034 coherence = -14.341240217410954\n",
      "tried 97 topics perplexity = -194.41510009765398 coherence = -14.341240217410954\n",
      "tried 98 topics perplexity = -196.39476013183258 coherence = -14.341240217410956\n",
      "tried 99 topics perplexity = -198.37483215331685 coherence = -14.341240217410956\n",
      "tried 100 topics perplexity = -200.35498046874812 coherence = -14.341240217410958\n"
     ]
    }
   ],
   "source": [
    "# Loop through a number of different topic model sizes\n",
    "\n",
    "results = pd.DataFrame()\n",
    "\n",
    "for num_topics in range(3, 101):\n",
    "\n",
    "    # Fit the lda model, with 10 topics\n",
    "    lda_model_tfidf = LdaModel(trainset['corpus_tfidf'],\n",
    "                               num_topics=num_topics,\n",
    "                               id2word=dictionary,\n",
    "                               passes=2)\n",
    "    \n",
    "    # Get the perplexity\n",
    "    perplexity = lda_model_tfidf.log_perplexity(testset['corpus_tfidf'])\n",
    "    \n",
    "    # Get the coherence\n",
    "    cm = CoherenceModel(model=lda_model_tfidf, corpus=testset['corpus_tfidf'], coherence='u_mass')\n",
    "    coherence = cm.get_coherence()\n",
    "    \n",
    "    # record\n",
    "    results = results.append({\"topics\":num_topics, \"perplexity\":perplexity, \"coherence\":coherence}, ignore_index=True)\n",
    "    \n",
    "    # Report for my convenience\n",
    "    print(\"tried {} topics\".format(num_topics), \"perplexity = {}\".format(perplexity), \"coherence = {}\".format(coherence))\n",
    "\n",
    "results.to_csv(\"tested_lda_stats_clean.csv\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7efd1cfd4c88>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the results\n",
    "sns.scatterplot(x=\"topics\", y=\"perplexity\", color=\"blue\", data=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7efd38793d30>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the results\n",
    "sns.scatterplot(x=\"topics\", y=\"coherence\", color=\"red\", data=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Create the \"Best\" Model!\n",
    "\n",
    "So what's the \"best\" model according to my two somewhat-arbitrary metrics?  First off it's worth noting that the metrics follow a consistent downward trend.  We are looking for a low perplexity score and a high coherence score.  Our other constraint is that I, personally, don't believe there can be more than 15 \"true\" varieties of wines!\n",
    "\n",
    "This last constraint is both a cheat that undermines the entire idea, and absolutely neccessary.  Unless you've got a very weirdly well-divided, well fleshed-out dataset of fairly long documents there's not going to be a \"true\" underlying topic model.  There'll be overlap in topic, and worse the topics can be fractal in nature, sub-dividing and making it difficult to know where to call it quits.\n",
    "\n",
    "Within this constraint, models with 9, 10, 11 or 12 topics display a consistently good coherence score suggesting a some persistence in discovered topics.  Over this range we also see something of a step-change in perplexity, implying a qualitative improvement over having smaller numbers of topics.\n",
    "\n",
    "I choose 11 topics - the value comes on the down-side of that possible step-change in perplexity, and has a locally better coherence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-d59ac769943c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                            \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                            \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                            passes=2)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Get the perplexity, out of curiosity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    978\u001b[0m                         \u001b[0mpass_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_no\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m                     )\n\u001b[0;32m--> 980\u001b[0;31m                     \u001b[0mgammat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_estep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_alpha\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mdo_estep\u001b[0;34m(self, chunk, state)\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m         \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollect_sstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msstats\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumdocs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# avoids calling len(chunk) on a generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    694\u001b[0m                 \u001b[0mElogthetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m                 \u001b[0mexpElogthetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mElogthetad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m                 \u001b[0mphinorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpElogthetad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpElogbetad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m                 \u001b[0;31m# If gamma hasn't changed much, we're done.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0mmeanchange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_difference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlastgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random.seed(10)\n",
    "\n",
    "# Fit the final lda model to all data\n",
    "lda_model_tfidf = LdaModel(wine_df['corpus_tfidf'],\n",
    "                           num_topics=10,\n",
    "                           id2word=dictionary,\n",
    "                           passes=2)\n",
    "\n",
    "# Get the perplexity, out of curiosity\n",
    "perplexity = lda_model_tfidf.log_perplexity(wine_df['corpus_tfidf'])\n",
    "    \n",
    "# Get the coherence, out of curiosity\n",
    "cm = CoherenceModel(model=lda_model_tfidf, corpus=wine_df['corpus_tfidf'], coherence='u_mass')\n",
    "coherence = cm.get_coherence()\n",
    "\n",
    "print(\"perplexity = {}\".format(perplexity), \"coherence = {}\".format(coherence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a quick look at the topics picked out\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So how did we do?\n",
    "Well;  looking at the words involved there are some valid-looking clusters there with some very apparently citrusy wines separated from aged, oakey wines!  This is a success!  There's a sweet wine cluster too with cherries, blackberries, \"soft\" and \"sweet\" flavours mentioned.  There's also annoyingly a weird topic that may have more to do with ancilliary descriptions of how wines are constructed - topic 1, describing stainless steel apperatus, and words like \"vineyard\" and \"ferment\".\n",
    "\n",
    "An obvious solution;  Simply ignore that topic during processing/assignment to clusters/visualisation!  Topic 0 may also be ancilliary details, mentioning \"cover\", \"salmon\", \"rest\" and \"extra\" - words that imply a link to serving suggestions.\n",
    "\n",
    "This is ultimately a useful lesson in the divergence of the latent topics of the corpus (on wine TASTING) from the topics we were seeking (on wine FLAVOUR).  A really useful way to examine the fit of what we've created is to utilise pyLDAvis, which uses PCA to do exactly what it says on the tin!\n",
    "\n",
    "And as it happens it supports the earlier conclusion;  in the first two principal components (and ignoring the part where it mucked up the topic indexes) the topics related to fermentation and serving suggestions appear well separated from the topics related to flavour.  Additionally, topic 9 is out of the way too, it talks of \"nose\", \"gravel\", and then a list of herbs that frankly sound like they'd go well with beef.  This decision, that these three topics are not like the rest and not relevant to our aims, is bourne out by the proportion of the corpus assigned to them.  Each of the three rogue topics contains 2-5 % of documents (roughly eyeballed), whereas the \"valid\" topics contain closer to 10 % each typically.\n",
    "\n",
    "### Final Conclusion\n",
    "The best-fit (low topic number) model has 11 topics.  Of those, three topics (index 0, 1 and 9) appear to be invalid, representing aspects of the descriptions less relevant to flavour.  This leaves 8 \"valid\" topics to be presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data = gensimvis.prepare(lda_model_tfidf, wine_df['corpus_tfidf'], dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf.save(\"./models/lda_model_tfidf_10_clean.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.  Save the results (and switch to R!)\n",
    "And for coursework reasons the visualisations of my brilliant work on wine must be presented using R.  I'll do some formatting and save the output topic models etc in some form R will like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = LdaModel.load(\"./models/lda_model_tfidf_14_clean.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a quick look at words the topics picked out\n",
    "topic_word_dist = pd.DataFrame()\n",
    "\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    \n",
    "    # Record the topic index in a format R will like\n",
    "    topic_index = \"X\" + str(idx)\n",
    "    \n",
    "    # Record the topic's key words as a single string\n",
    "    # Split the words with a newline character while we're at it!\n",
    "    topic_words = \" \".join([x.split(\"*\")[1].replace('\"', '').strip() for x in topic.split(\"+\")][0:5])\n",
    "    \n",
    "    # Dump them to the results dataframe\n",
    "    topic_word_dist = topic_word_dist.append({\"topic\":topic_index, \"keywords\":topic_words}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>menthol framework polish truffl scorch</td>\n",
       "      <td>X0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crisp fresh fruiti light acid</td>\n",
       "      <td>X1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ferment oak vineyard barrel french</td>\n",
       "      <td>X2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>appl peach lemon white citru</td>\n",
       "      <td>X3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pungent stylish crowd brut barbera</td>\n",
       "      <td>X4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>age structur wood rich fruit</td>\n",
       "      <td>X5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>berri finish plum feel aroma</td>\n",
       "      <td>X6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>futur stage term promis touriga</td>\n",
       "      <td>X7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>black cherri palat tannin aroma</td>\n",
       "      <td>X8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pinot cherri noir raspberri sweet</td>\n",
       "      <td>X9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cabernet sauvignon merlot franc blend</td>\n",
       "      <td>X10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mango chardonnay butterscotch pineappl honeysuckl</td>\n",
       "      <td>X11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>steeli sampl half initi contrast</td>\n",
       "      <td>X12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>meat black blackberri pepper cherri</td>\n",
       "      <td>X13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             keywords topic\n",
       "0              menthol framework polish truffl scorch    X0\n",
       "1                       crisp fresh fruiti light acid    X1\n",
       "2                  ferment oak vineyard barrel french    X2\n",
       "3                        appl peach lemon white citru    X3\n",
       "4                  pungent stylish crowd brut barbera    X4\n",
       "5                        age structur wood rich fruit    X5\n",
       "6                        berri finish plum feel aroma    X6\n",
       "7                     futur stage term promis touriga    X7\n",
       "8                     black cherri palat tannin aroma    X8\n",
       "9                   pinot cherri noir raspberri sweet    X9\n",
       "10              cabernet sauvignon merlot franc blend   X10\n",
       "11  mango chardonnay butterscotch pineappl honeysuckl   X11\n",
       "12                   steeli sampl half initi contrast   X12\n",
       "13                meat black blackberri pepper cherri   X13"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_dist.to_csv(\"./data/topic_14_word_dist_5_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topics = [ [x[1] for x in doc] for doc in lda_model_tfidf.get_document_topics(wine_df['corpus_tfidf']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.01474163,\n",
       "  0.014741495,\n",
       "  0.014741493,\n",
       "  0.5686674,\n",
       "  0.25443414,\n",
       "  0.014741481,\n",
       "  0.0147416,\n",
       "  0.014741466,\n",
       "  0.014741563,\n",
       "  0.014741538,\n",
       "  0.014741563,\n",
       "  0.014741551,\n",
       "  0.014741462,\n",
       "  0.014741563],\n",
       " [0.015796667,\n",
       "  0.015796753,\n",
       "  0.01579671,\n",
       "  0.015796682,\n",
       "  0.015796678,\n",
       "  0.5950971,\n",
       "  0.015796695,\n",
       "  0.015796969,\n",
       "  0.01579671,\n",
       "  0.015797041,\n",
       "  0.015796702,\n",
       "  0.015796669,\n",
       "  0.21534188,\n",
       "  0.015796708],\n",
       " [0.015462183,\n",
       "  0.015462336,\n",
       "  0.7989904,\n",
       "  0.015462567,\n",
       "  0.015462398,\n",
       "  0.01546227,\n",
       "  0.015462353,\n",
       "  0.01546219,\n",
       "  0.015462212,\n",
       "  0.015462209,\n",
       "  0.015462201,\n",
       "  0.0154622635,\n",
       "  0.0154621815,\n",
       "  0.015462195],\n",
       " [0.12701835,\n",
       "  0.013494654,\n",
       "  0.013494672,\n",
       "  0.6605643,\n",
       "  0.013494609,\n",
       "  0.013494597,\n",
       "  0.01349488,\n",
       "  0.013494577,\n",
       "  0.013495073,\n",
       "  0.01349492,\n",
       "  0.013494522,\n",
       "  0.06397552,\n",
       "  0.013494659,\n",
       "  0.013494687],\n",
       " [0.014621315,\n",
       "  0.014621309,\n",
       "  0.18898721,\n",
       "  0.014621269,\n",
       "  0.014622563,\n",
       "  0.014621214,\n",
       "  0.45003653,\n",
       "  0.067778595,\n",
       "  0.014621536,\n",
       "  0.014621297,\n",
       "  0.14698295,\n",
       "  0.014621338,\n",
       "  0.014621228,\n",
       "  0.014621681]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topics[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_df = pd.DataFrame(doc_topics)\n",
    "doc_topic_df['title'] = wine_df['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.014742</td>\n",
       "      <td>0.014741</td>\n",
       "      <td>0.014741</td>\n",
       "      <td>0.568667</td>\n",
       "      <td>0.254434</td>\n",
       "      <td>0.014741</td>\n",
       "      <td>0.014742</td>\n",
       "      <td>0.014741</td>\n",
       "      <td>0.014742</td>\n",
       "      <td>0.014742</td>\n",
       "      <td>0.014742</td>\n",
       "      <td>0.014742</td>\n",
       "      <td>0.014741</td>\n",
       "      <td>0.014742</td>\n",
       "      <td>Nicosia 2013 VulkÃ  Bianco  (Etna)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.595097</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.215342</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>Quinta dos Avidagos 2011 Avidagos Red (Douro)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015462</td>\n",
       "      <td>0.015462</td>\n",
       "      <td>0.798990</td>\n",
       "      <td>0.015463</td>\n",
       "      <td>0.015462</td>\n",
       "      <td>0.015462</td>\n",
       "      <td>0.015462</td>\n",
       "      <td>0.015462</td>\n",
       "      <td>0.015462</td>\n",
       "      <td>0.015462</td>\n",
       "      <td>0.015462</td>\n",
       "      <td>0.015462</td>\n",
       "      <td>0.015462</td>\n",
       "      <td>0.015462</td>\n",
       "      <td>Rainstorm 2013 Pinot Gris (Willamette Valley)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.127018</td>\n",
       "      <td>0.013495</td>\n",
       "      <td>0.013495</td>\n",
       "      <td>0.660564</td>\n",
       "      <td>0.013495</td>\n",
       "      <td>0.013495</td>\n",
       "      <td>0.013495</td>\n",
       "      <td>0.013495</td>\n",
       "      <td>0.013495</td>\n",
       "      <td>0.013495</td>\n",
       "      <td>0.013495</td>\n",
       "      <td>0.063976</td>\n",
       "      <td>0.013495</td>\n",
       "      <td>0.013495</td>\n",
       "      <td>St. Julian 2013 Reserve Late Harvest Riesling ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.014621</td>\n",
       "      <td>0.014621</td>\n",
       "      <td>0.188987</td>\n",
       "      <td>0.014621</td>\n",
       "      <td>0.014623</td>\n",
       "      <td>0.014621</td>\n",
       "      <td>0.450037</td>\n",
       "      <td>0.067779</td>\n",
       "      <td>0.014622</td>\n",
       "      <td>0.014621</td>\n",
       "      <td>0.146983</td>\n",
       "      <td>0.014621</td>\n",
       "      <td>0.014621</td>\n",
       "      <td>0.014622</td>\n",
       "      <td>Sweet Cheeks 2012 Vintner's Reserve Wild Child...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.014742  0.014741  0.014741  0.568667  0.254434  0.014741  0.014742   \n",
       "1  0.015797  0.015797  0.015797  0.015797  0.015797  0.595097  0.015797   \n",
       "2  0.015462  0.015462  0.798990  0.015463  0.015462  0.015462  0.015462   \n",
       "3  0.127018  0.013495  0.013495  0.660564  0.013495  0.013495  0.013495   \n",
       "4  0.014621  0.014621  0.188987  0.014621  0.014623  0.014621  0.450037   \n",
       "\n",
       "          7         8         9        10        11        12        13  \\\n",
       "0  0.014741  0.014742  0.014742  0.014742  0.014742  0.014741  0.014742   \n",
       "1  0.015797  0.015797  0.015797  0.015797  0.015797  0.215342  0.015797   \n",
       "2  0.015462  0.015462  0.015462  0.015462  0.015462  0.015462  0.015462   \n",
       "3  0.013495  0.013495  0.013495  0.013495  0.063976  0.013495  0.013495   \n",
       "4  0.067779  0.014622  0.014621  0.146983  0.014621  0.014621  0.014622   \n",
       "\n",
       "                                               title  \n",
       "0                  Nicosia 2013 VulkÃ  Bianco  (Etna)  \n",
       "1      Quinta dos Avidagos 2011 Avidagos Red (Douro)  \n",
       "2      Rainstorm 2013 Pinot Gris (Willamette Valley)  \n",
       "3  St. Julian 2013 Reserve Late Harvest Riesling ...  \n",
       "4  Sweet Cheeks 2012 Vintner's Reserve Wild Child...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = wine_df.merge(doc_topic_df, on=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aromas include tropical fruit, broom, brimston...</td>\n",
       "      <td>[aroma, includ, tropic, fruit, broom, brimston...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is ripe and fruity, a wine that is smooth...</td>\n",
       "      <td>[ripe, fruiti, wine, smooth, structur, firm, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tart and snappy, the flavors of lime flesh and...</td>\n",
       "      <td>[tart, snappi, flavor, lime, flesh, rind, domi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pineapple rind, lemon pith and orange blossom ...</td>\n",
       "      <td>[pineappl, rind, lemon, pith, orang, blossom, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Much like the regular bottling from 2012, this...</td>\n",
       "      <td>[like, regular, bottl, come, rough, tannic, ru...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  Aromas include tropical fruit, broom, brimston...   \n",
       "1  This is ripe and fruity, a wine that is smooth...   \n",
       "2  Tart and snappy, the flavors of lime flesh and...   \n",
       "3  Pineapple rind, lemon pith and orange blossom ...   \n",
       "4  Much like the regular bottling from 2012, this...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [aroma, includ, tropic, fruit, broom, brimston...  \n",
       "1  [ripe, fruiti, wine, smooth, structur, firm, t...  \n",
       "2  [tart, snappi, flavor, lime, flesh, rind, domi...  \n",
       "3  [pineappl, rind, lemon, pith, orang, blossom, ...  \n",
       "4  [like, regular, bottl, come, rough, tannic, ru...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[['description', 'tokens']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"./data/wine_reviews_with_14_topic_distributions_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
